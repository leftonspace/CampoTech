# ═══════════════════════════════════════════════════════════════════════════════
# PROMETHEUS ALERTING RULES
# CampoTech Field Service Management
# ═══════════════════════════════════════════════════════════════════════════════

groups:
  # ═══════════════════════════════════════════════════════════════════════════════
  # APPLICATION HEALTH
  # ═══════════════════════════════════════════════════════════════════════════════
  - name: application_health
    rules:
      # API Server Down
      - alert: APIServerDown
        expr: up{job="campotech-api"} == 0
        for: 1m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "API server is down"
          description: "API server {{ $labels.instance }} has been down for more than 1 minute."
          runbook_url: "https://wiki.campotech.com/runbooks/api-down"

      # High Error Rate
      - alert: HighErrorRate
        expr: |
          sum(rate(http_requests_total{status=~"5.."}[5m])) /
          sum(rate(http_requests_total[5m])) > 0.05
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "High HTTP error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"

      # Critical Error Rate
      - alert: CriticalErrorRate
        expr: |
          sum(rate(http_requests_total{status=~"5.."}[5m])) /
          sum(rate(http_requests_total[5m])) > 0.10
        for: 2m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "Critical HTTP error rate"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 10%)"

      # High Latency
      - alert: HighLatency
        expr: |
          histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le)) > 2
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "High API latency detected"
          description: "P95 latency is {{ $value | humanizeDuration }}"

      # Critical Latency
      - alert: CriticalLatency
        expr: |
          histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le)) > 5
        for: 2m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "Critical API latency"
          description: "P95 latency is {{ $value | humanizeDuration }}"

  # ═══════════════════════════════════════════════════════════════════════════════
  # DATABASE
  # ═══════════════════════════════════════════════════════════════════════════════
  - name: database
    rules:
      # Database Connection Pool Exhausted
      - alert: DatabaseConnectionPoolExhausted
        expr: pg_stat_activity_count > pg_settings_max_connections * 0.9
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "Database connection pool nearly exhausted"
          description: "{{ $value }} connections used of {{ $labels.max_connections }} max"

      # Database Down
      - alert: DatabaseDown
        expr: pg_up == 0
        for: 1m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "PostgreSQL database is down"
          description: "Database {{ $labels.instance }} is unreachable"

      # Slow Queries
      - alert: SlowQueries
        expr: |
          rate(pg_stat_statements_seconds_total[5m]) /
          rate(pg_stat_statements_calls_total[5m]) > 1
        for: 10m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "Slow database queries detected"
          description: "Average query time is {{ $value | humanizeDuration }}"

      # Replication Lag
      - alert: DatabaseReplicationLag
        expr: pg_replication_lag > 30
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "Database replication lag detected"
          description: "Replication lag is {{ $value }}s"

  # ═══════════════════════════════════════════════════════════════════════════════
  # REDIS
  # ═══════════════════════════════════════════════════════════════════════════════
  - name: redis
    rules:
      # Redis Down
      - alert: RedisDown
        expr: redis_up == 0
        for: 1m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Redis is down"
          description: "Redis instance {{ $labels.instance }} is unreachable"

      # Redis Memory High
      - alert: RedisMemoryHigh
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "Redis memory usage high"
          description: "Redis memory usage is {{ $value | humanizePercentage }}"

      # Redis Connected Clients High
      - alert: RedisConnectedClientsHigh
        expr: redis_connected_clients > 500
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "High number of Redis connections"
          description: "{{ $value }} clients connected to Redis"

  # ═══════════════════════════════════════════════════════════════════════════════
  # QUEUE PROCESSING
  # ═══════════════════════════════════════════════════════════════════════════════
  - name: queues
    rules:
      # Queue Backlog High
      - alert: QueueBacklogHigh
        expr: bullmq_queue_waiting > 100
        for: 10m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "High queue backlog"
          description: "Queue {{ $labels.queue }} has {{ $value }} waiting jobs"

      # Queue Backlog Critical
      - alert: QueueBacklogCritical
        expr: bullmq_queue_waiting > 500
        for: 5m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "Critical queue backlog"
          description: "Queue {{ $labels.queue }} has {{ $value }} waiting jobs"

      # Failed Jobs High
      - alert: FailedJobsHigh
        expr: increase(bullmq_queue_failed[1h]) > 50
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "High number of failed jobs"
          description: "{{ $value }} jobs failed in the last hour on queue {{ $labels.queue }}"

      # Worker Down
      - alert: QueueWorkerDown
        expr: bullmq_worker_active == 0
        for: 5m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "Queue worker is down"
          description: "No active workers for queue {{ $labels.queue }}"

  # ═══════════════════════════════════════════════════════════════════════════════
  # EXTERNAL INTEGRATIONS
  # ═══════════════════════════════════════════════════════════════════════════════
  - name: integrations
    rules:
      # AFIP Integration Errors
      - alert: AFIPIntegrationErrors
        expr: increase(afip_requests_failed_total[15m]) > 5
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "AFIP integration errors"
          description: "{{ $value }} AFIP requests failed in the last 15 minutes"

      # MercadoPago Integration Errors
      - alert: MercadoPagoErrors
        expr: increase(mercadopago_requests_failed_total[15m]) > 5
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "MercadoPago integration errors"
          description: "{{ $value }} MercadoPago requests failed"

      # WhatsApp Integration Errors
      - alert: WhatsAppErrors
        expr: increase(whatsapp_messages_failed_total[15m]) > 10
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "WhatsApp message delivery errors"
          description: "{{ $value }} WhatsApp messages failed"

      # WhatsApp Panic Mode
      - alert: WhatsAppPanicMode
        expr: whatsapp_panic_mode == 1
        for: 1m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "WhatsApp integration in panic mode"
          description: "WhatsApp integration has been disabled due to high failure rate"

      # OpenAI API Errors
      - alert: OpenAIErrors
        expr: increase(openai_requests_failed_total[15m]) > 10
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "OpenAI API errors"
          description: "{{ $value }} OpenAI requests failed"

  # ═══════════════════════════════════════════════════════════════════════════════
  # INFRASTRUCTURE
  # ═══════════════════════════════════════════════════════════════════════════════
  - name: infrastructure
    rules:
      # High CPU Usage
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 10m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "High CPU usage"
          description: "CPU usage is {{ $value | printf \"%.1f\" }}% on {{ $labels.instance }}"

      # High Memory Usage
      - alert: HighMemoryUsage
        expr: |
          (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 10m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value | printf \"%.1f\" }}% on {{ $labels.instance }}"

      # Disk Space Low
      - alert: DiskSpaceLow
        expr: |
          (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 85
        for: 10m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "Low disk space"
          description: "Disk usage is {{ $value | printf \"%.1f\" }}% on {{ $labels.instance }}"

      # Disk Space Critical
      - alert: DiskSpaceCritical
        expr: |
          (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 95
        for: 5m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Critical disk space"
          description: "Disk usage is {{ $value | printf \"%.1f\" }}% on {{ $labels.instance }}"

  # ═══════════════════════════════════════════════════════════════════════════════
  # BUSINESS METRICS
  # ═══════════════════════════════════════════════════════════════════════════════
  - name: business
    rules:
      # No Jobs Created
      - alert: NoJobsCreated
        expr: increase(jobs_created_total[1h]) == 0
        for: 2h
        labels:
          severity: warning
          team: product
        annotations:
          summary: "No jobs created in 2 hours"
          description: "No new jobs have been created in the last 2 hours during business hours"

      # Invoice Generation Failures
      - alert: InvoiceGenerationFailures
        expr: increase(invoices_failed_total[1h]) > 5
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "Invoice generation failures"
          description: "{{ $value }} invoices failed to generate in the last hour"

      # Payment Processing Failures
      - alert: PaymentProcessingFailures
        expr: increase(payments_failed_total[1h]) > 3
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "Payment processing failures"
          description: "{{ $value }} payments failed to process"
